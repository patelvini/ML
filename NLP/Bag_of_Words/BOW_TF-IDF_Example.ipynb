{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement TF-IDF scheme from scratch in python\n",
    "\n",
    "The term TF stands for \"term frequency\" while the term IDF stands for the \"inverse document frequency\".\n",
    "\n",
    "## Problem with Bag of Words Model\n",
    "\n",
    "Before we actually see the TF-IDF model, let us first discuss a few problems associated with the bag of words model.\n",
    "\n",
    "for Example,\n",
    "we had the following three example sentences:\n",
    "\n",
    "- \"I like to play football\"\n",
    "- \"Did you go outside to play tennis\"\n",
    "- \"John and I play tennis\"\n",
    "\n",
    "The resulting bag of words model looked like this:\n",
    "\n",
    "![](BOW_Model.PNG)\n",
    "\n",
    "One of the main problems associated with the bag of words model is that it assigns equal value to the words, irrespective of their importance. For instance, the word **\"play\"** appears in all the three sentences, therefore this word is very common, on the other hand, the word **\"football\"** only appears in one sentence. The words that are rare have more classifying power compared to the words that are common.\n",
    "\n",
    "The idea behind the `TF-IDF` approach is that the words that are more common in one sentence and less common in other sentences should be given high weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Behind TF-IDF\n",
    "\n",
    "Before implementing TF-IDF scheme in Python, let's first study the theory. \n",
    "\n",
    "For example,\n",
    "\n",
    "- \"I like to play football\"\n",
    "- \"Did you go outside to play tennis\"\n",
    "- \"John and I play tennis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: Tokenization\n",
    "\n",
    "Like the bag of words, the first step to implement TF-IDF model, is tokenization.\n",
    "\n",
    "![](Tokenization.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: Find TF-IDF Values\n",
    "\n",
    "Once you have tokenized the sentences, the next step is to find the TF-IDF value for each word in the sentence.\n",
    "\n",
    "TF value refers to term frequency and can be calculated as follows:\n",
    "\n",
    "**TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)**\n",
    "\n",
    "For instance, look at the word \"play\" in the first sentence. Its term frequency will be 0.20 since the word \"play\" occurs only once in the sentence and the total number of words in the sentence are 5, hence, 1/5 = 0.20.\n",
    "\n",
    "IDF refers to inverse document frequency and can be calculated as follows:\n",
    "\n",
    "**IDF = (Total number of sentences (documents))/(Number of sentences (documents) containing the word)**\n",
    "\n",
    "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n",
    "\n",
    "Let's find the IDF frequency of the word \"play\". Since we have three documents and the word \"play\" occurs in all three of them, therefore the IDF value of the word \"play\" is 3/3 = 1.\n",
    "\n",
    "Finally, the TF-IDF values are calculated by multiplying TF values with their corresponding IDF values.\n",
    "\n",
    "To find the TF-IDF value, we first need to create a dictionary of word frequencies as shown below:\n",
    "\n",
    "![](word_frequencies.PNG)\n",
    "\n",
    "Next, let's sort the dictionary in the descending order of the frequency as shown in the following table.\n",
    "\n",
    "![](word_frequencies_ordered.PNG)\n",
    "\n",
    "Finally, we will filter the 8 most frequently occurring words.\n",
    "\n",
    "As I said earlier, since IDF values are calculated using the whole corpus. We can calculate the IDF value for each word now. The following table contains IDF values for each table.\n",
    "\n",
    "| Word | Frequency | IDF |\n",
    "|--|--|--|\n",
    "| play | 3 | 3/3 = 1 |\n",
    "| tennis | 2 | 3/2 = 1.5 |\n",
    "| to | 2 | 3/2 = 1.5 |\n",
    "| I\t| 2 | 3/2 = 1.5 |\n",
    "| football | 1 | 3/1 = 3 |\n",
    "| Did | 1 | 3/1 = 3 |\n",
    "| you | 1 | 3/1 = 3 |\n",
    "| go | 1 | 3/1 = 3 |\n",
    "\n",
    "You can clearly see that the words that are rare have higher IDF values compared to the words that are more common.\n",
    "\n",
    "Let's now find the TF-IDF values for all the words in each sentence.\n",
    "\n",
    "| Word | Sentence 1 | Sentence 2 | Sentence 3 |\n",
    "|--|--|--|--|\n",
    "| play | 0.20 x 1 = 0.20 | 0.14 x 1 = 0.14 | 0.20 x 1 = 0.20 |\n",
    "| tennis | 0 x 1.5 = 0 | 0.14 x 1.5 = 0.21 | 0.20 x 1.5 = 0.30 |\n",
    "|  to | 0.20 x 1.5 = 0.30 | 0.14 x 1.5 = 0.21 | 0 x 1.5 = 0 |\n",
    "|  I | 0.20 x 1.5 = 0.30 | 0 x 1.5 = 0 | 0.20 x 1.5 = 0.30 |\n",
    "|  football | 0.20 x 3 = 0.6 | 0 x 3 = 0 | 0 x 3 = 0 |\n",
    "|  did | 0 x 3 = 0 | 0.14 x 3 = 0.42 | 0 x 3 = 0 |\n",
    "|  you | 0 x3 = 0 | 0.14 x 3 = 0.42 | 0 x 3 = 0 |\n",
    "|  go | 0x 3 = 0 | 0.14 x 3 = 0.42 | 0 x 3 = 0 |\n",
    "\n",
    "The values in the columns for sentence 1, 2, and 3 are corresponding TF-IDF vectors for each word in the respective sentences.\n",
    "\n",
    "Note the use of the log function with TF-IDF.\n",
    "\n",
    "It is important to mention that to mitigate the effect of very rare and very common words on the corpus, the log of the IDF value can be calculated before multiplying it with the TF-IDF value. In such case the formula of IDF becomes:\n",
    "\n",
    "**IDF = log((Total number of sentences (documents))/(Number of sentences (documents) containing the word))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Model from Scratch in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "html_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "\n",
    "html_data = html_data.read()\n",
    "\n",
    "article = bs.BeautifulSoup(html_data, 'lxml')\n",
    "\n",
    "article_paragraphs = article.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for paragraph in article_paragraphs:\n",
    "    article_text += paragraph.text\n",
    "\n",
    "corpus = nltk.sent_tokenize(article_text)\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    corpus[i] = re.sub(r'\\W',' ',corpus[i])\n",
    "    corpus[i] = re.sub(r'\\s+',' ',corpus[i])\n",
    "    \n",
    "wordfreq = {}\n",
    "\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1\n",
    "            \n",
    "import heapq\n",
    "\n",
    "most_freq = heapq.nlargest(200, wordfreq, key = wordfreq.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above script, we first scrape the Wikipedia article on Natural Language Processing. We then pre-process it to remove all the special characters and multiple empty spaces. Finally, we create a dictionary of word frequencies and then filter the top 200 most frequently occurring words.\n",
    "\n",
    "The next step is to find the IDF values for the most frequently occurring words in the corpus. The following script does that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in corpus:\n",
    "        if token in nltk.word_tokenize(document):\n",
    "            doc_containing_word += 1\n",
    "            \n",
    "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the script above, we create an empty dictionary `word_idf_values`. This dictionary will store most frequently occurring words as keys and their corresponding IDF values as dictionary values. Next, we iterate through the list of most frequently occurring words. During each iteration, we create a variable `doc_containing_word`. This variable will store the number of documents in which the word appears. Next, we iterate through all the sentences in our corpus. The sentence is tokenized and then we check if the word exists in the sentence or not, if the word exists, we increment the `doc_containing_word` variable. Finally, to calculate the IDF value we divide the total number of sentences by the total number of documents containing the word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the TF dictionary for each word. In the TF dictionary, the key will be the most frequently occuring words, while values will be 47 dimensional vectors since our document has 47 sentences. Each value in the vector will belong to the TF value of the word for the corresponding sentence. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in corpus:\n",
    "        doc_freq = 0\n",
    "        for word in nltk.word_tokenize(document):\n",
    "            if token == word:\n",
    "                  doc_freq += 1\n",
    "        word_tf = doc_freq/len(nltk.word_tokenize(document))\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above script, we create a dictionary that contains the word as the key and a list of 49 items as a value since we have 49 sentences in our corpus. Each item in the list stores the TF value of the word for the corresponding sentence. In the script above `word_tf_values` is our dictionary. For each word, we create a list `sent_tf_vector`.\n",
    "\n",
    "We then iterate through each sentence in the corpus and tokenize the sentence. The word from the outer loop is matched with each word in the sentence. If a match is found the `doc_freq` variable is incremented by 1. Once, all the words in the sentence are iterated, the `doc_freq` is divided by the total length of the sentence to find the TF value of the word for that sentence. This process repeats for all the words in the most frequently occurring word list. The final `word_tf_values` dictionary will contain 200 words as keys. For each word, there will be a list of 47 items as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k,v in word_tf_values.items():\n",
    "#    print(f'{k} -> {v}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have IDF values of all the words, along with TF values of every word across the sentences. The next step is to simply multiply IDF values with TF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = []\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above script, we create a list called `tfidf_values`. We then iterated through all the keys in the `word_tf_values` dictionary. These keys are basically the most frequently occurring words. Using these words, we retrieve the 47-dimensional list that contains the TF values for the word corresponding to each sentence. Next, the TF value is multiplied by the IDF value of the word and stored in the `tf_idf_score` variable. The variable is then appended to the `tf_idf_sentences` list. Finally, the `tf_idf_sentences` list is appended to the `tfidf_values` list.\n",
    "\n",
    "Now at this point in time, the `tfidf_value`s is a list of lists. Where each item is a 47-dimensional list that contains TFIDF values of a particular word for all the sentences. We need to convert the two-dimensional list to a numpy array. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = np.asarray(tfidf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00770922, 0.        , 0.03237871, ..., 0.        , 0.        ,\n",
       "        0.0281554 ],\n",
       "       [0.02137858, 0.        , 0.02244751, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01503028, 0.        , 0.        , ..., 0.        , 0.10521196,\n",
       "        0.0274466 ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 47)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is still one problem with this TF-IDF model. The array dimension is 200 x 47, which means that each column represents the TF-IDF vector for the corresponding sentence. We want rows to represent the TF-IDF vectors. We can do so by simply transposing our numpy array as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = np.transpose(tf_idf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "TF-IDF model is one of the most widely used models for text to numeric conversion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
